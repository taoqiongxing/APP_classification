{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('C:/1/app_data_new.csv',sep=None,encoding='utf8',engine = 'python')\n",
    "\n",
    "def dt_remove(data):\n",
    "    pattern = u'[a-zA-Z0-9]+'\n",
    "    wd_length = len(re.findall(pattern,data))\n",
    "    return wd_length\n",
    "\n",
    "data['app_tags'].fillna('missing',inplace=True)\n",
    "\n",
    "pd_dt = data[(data['len']==0)]\n",
    "pd_dt.to_csv('app_data_new.csv',encoding='utf-8',sep=' ')\n",
    "\n",
    "app_tags = []\n",
    "for d in pd_dt['app_tags']:\n",
    "    app_tag = d.strip().split()\n",
    "    for x in app_tag:\n",
    "        app_tags.append(x)\n",
    "        \n",
    "app_tags = list(set(app_tags))\n",
    "apps = []\n",
    "\n",
    "for d in pd_dt['app_tags']:\n",
    "    apps.append(d.split())\n",
    "    \n",
    "def get_zero_list():\n",
    "    app_list = []\n",
    "    for i in range(len(app_tags)):\n",
    "        app_list.append(0)\n",
    "    return app_list\n",
    "\n",
    "app_list = get_zero_list()\n",
    "app_lists = []\n",
    "for app in apps:\n",
    "    app_list = get_zero_list()\n",
    "    for a in app:\n",
    "        for x in app_tags:\n",
    "            if(x == a):\n",
    "                app_list[app_tags.index(x)]=1\n",
    "    app_lists.append(app_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_array = np.array(app_lists)##########################y\n",
    "app_discribe = pd_dt['app_discribe']\n",
    "app_example = app_discribe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# 创建停用词list  \n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  \n",
    "    return stopwords \n",
    "\n",
    "def dt_remove(data):\n",
    "    pattern = u'[a-zA-Z0-9]+'\n",
    "    wd_length = len(re.findall(pattern,data))\n",
    "    return wd_length\n",
    "\n",
    "# 对句子进行分词  \n",
    "def seg_sentence(sentence):  \n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    # 这里加载停用词的路径\n",
    "    stopwords = stopwordslist('C:/1/stopwords.txt')   \n",
    "    outstr = []\n",
    "    for word in sentence_seged:  \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t': \n",
    "                if word != '\\xa0':\n",
    "                    if dt_remove(word) ==0:\n",
    "                        outstr.append(word) \n",
    "    return outstr  \n",
    "\n",
    "app_discribe_list = []\n",
    "for app in app_discribe:\n",
    "    app_discribe_list.append(seg_sentence(app))\n",
    "    \n",
    "' '.join(app_discribe_list[100])\n",
    "\n",
    "with open('app_discribe_cut_list.txt','w',encoding='utf-8') as output:\n",
    "    for app in app_discribe_list:   \n",
    "        output.write(' '.join(app) + '\\n')\n",
    "        \n",
    "y = np.array(label_array)################y\n",
    "sentences = app_discribe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y == label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "\n",
    "model = word2vec.Word2Vec(size=300,workers=multiprocessing.cpu_count())\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences,total_examples=model.corpus_count,epochs=model.iter)\n",
    "model.save('Word2vec_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.preprocessing import sequence\n",
    "gensim_dict = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(\n",
    "                            model.wv.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=100)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print('No data provided...')\n",
    "        \n",
    "index_dict, word_vectors,combined = create_dictionaries(model=model,combined=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def get_data(index_dict,word_vectors,combined,y):\n",
    "\n",
    "    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    embedding_weights = np.zeros((n_symbols, 300))#索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items():#从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "    print(x_train.shape,y_train.shape)\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test\n",
    "\n",
    "n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,combined,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "import yaml\n",
    "\n",
    "##定义网络结构\n",
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    print('Defining a Simple Keras Model...')\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(output_dim=300,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=100))  # Adding Input Length\n",
    "    model.add(LSTM(output_dim=100, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4703))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print('Compiling the Model...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print(\"Train...\")\n",
    "    model.fit(x_train, y_train, batch_size=100, nb_epoch=4,verbose=1, shuffle=True,validation_split=0.1)\n",
    "\n",
    "    print(\"Evaluate...\")\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                                batch_size=100)\n",
    "\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('lstm.yml', 'w') as outfile:\n",
    "        outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "    model.save_weights('lstm.h5')\n",
    "    print('Test score:', score)\n",
    "    \n",
    "train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_app = \"QQ•乐在沟通----- √服务超过90%的移动互联网用户 √多人视频、文件多端互传，不断创新满足沟通所需 √致力于打造欢乐无限的沟通、娱乐与生活体验-----主要功能----- •聊天消息：随时随地收发好友和群消息，一触即达。 •语音通话：两人、多人语音通话，高清畅聊。 •视频聊天：亲朋好友，想念不如相见。•文件传输：手机、电脑多端互传，方便快捷。 •空间动态：更快获厘 米 秀：换装扮、炫动作、偷胶囊，年轻人最爱的潮爆玩法。•个性装扮：主题、名片、彩铃、气泡、挂件自由选。 •游戏中心：天天、全民等最热手游，根本停不下来。 •移动支付：话费充值、网购转账收款，一应俱全。 •QQ看点：专为年轻人打造的个性化内容推荐平台。乐在沟通18年，聊天欢乐9亿人！ -----联系我们----- 如在使用过程中遇到任何问题，请联系我们：- 在线帮助：进入QQ设置 -> 关于QQ -> 帮助与反馈 - 客服热线：0755 -83763333（服务时间：8:00 - 23:00）更新内容：【本次更新】 - 可录制与好友的视频通话过程并保存，留下精彩片段随时查看； - 两人视频通话时可选择换脸挂件，聊天时脸部互换，喜感十足； - 修复部分场景下的机型兼容问题，加强版本稳定性。【更多优化】 - 短视频渲染性能优化，短视频录制更流畅； - QQ表情性能提升，内存空间占用更小，表情显示更加清晰。\"\n",
    "test_app=\"也许你和小编一样每天都要面对大量各类琐事，生活上、工作上，一款既轻巧又实用的日程安排工具是必不可少的，那么现在我就推荐给大家一款目前小编觉得最好用的Any.Do。流畅的使用体验，极简的设计风格，让它成为了计划管理类应用的最佳选择之一。\"\n",
    "app_cut_list = seg_sentence(test_app)\n",
    "print(app_cut_list)\n",
    "app_cut = []\n",
    "for app in app_cut_list:\n",
    "    if app != '•':\n",
    "        if app != '√':\n",
    "            if app != ' ':\n",
    "                app_cut.append(app)\n",
    "\n",
    "words = np.array(app_cut).reshape(1,-1)\n",
    "model=word2vec.Word2Vec.load('Word2vec_model.pkl')\n",
    "_,_,combined=create_dictionaries(model,words)\n",
    "\n",
    "with open('lstm.yml', 'r') as f:\n",
    "    yaml_string = yaml.load(f)\n",
    "model = model_from_yaml(yaml_string)\n",
    "\n",
    "print('loading weights......')\n",
    "model.load_weights('lstm.h5')\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "data=combined\n",
    "data.reshape(1,-1)\n",
    "\n",
    "#print data\n",
    "result=model.predict_classes(data)\n",
    "result = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "g = heapq.nlargest(10,result[0])\n",
    "i = 0\n",
    "predict = []\n",
    "for r in result[0]:\n",
    "    if r in g:\n",
    "        predict.append(app_tags[i])\n",
    "    i = i+1\n",
    "    \n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
