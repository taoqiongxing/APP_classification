{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import jieba\n",
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "data = pd.read_csv('app_data_new.csv',sep=None,encoding='utf8',engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_tags_list = data['app_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#APP标签\n",
    "app_tags = []\n",
    "for d in app_tags_list:\n",
    "    app_tag = d.strip().split()\n",
    "    for x in app_tag:\n",
    "        app_tags.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#标签去重\n",
    "app_tags = list(set(app_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义函数增0列\n",
    "def get_zero_list():\n",
    "    zero_list = []\n",
    "    for i in range(len(app_tags)):\n",
    "        zero_list.append(0)\n",
    "    return zero_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#APP标签0-1向量化为列表\n",
    "app_tags_matrix = []\n",
    "zero_list = get_zero_list()\n",
    "for a in app_tags_list:\n",
    "    zero_list = get_zero_list()\n",
    "    for per_tag in a:\n",
    "        for app_tag in app_tags:\n",
    "            if(per_tag == app_tag):\n",
    "                zero_list[app_tags.index(app_tag)] = 1\n",
    "    app_tags_matrix.append(zero_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#APP描述\n",
    "app_discribe = data['app_discribe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建停用词列表\n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  \n",
    "    return stopwords \n",
    "def dt_remove(data):\n",
    "    pattern = u'[a-zA-Z0-9]+'\n",
    "    wd_length = len(re.findall(pattern,data))\n",
    "    return wd_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jieba分词\n",
    "def seg_sentence(sentence):  \n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stopwords = stopwordslist('stopwords.txt')   \n",
    "    outstr = []\n",
    "    for word in sentence_seged:  \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t': \n",
    "                if word != '\\xa0':\n",
    "                    if dt_remove(word) ==0:\n",
    "                        outstr.append(word) \n",
    "    return outstr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Ax\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.571 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#对APP描述进行分词\n",
    "app_discribe_list = []\n",
    "for app in app_discribe:\n",
    "    app_discribe_list.append(seg_sentence(app))\n",
    "with open('app_discribe_cut_list.txt','w',encoding='utf-8') as output:\n",
    "    for app in app_discribe_list: \n",
    "        output.write(' '.join(app) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(app_discribe_list,min_count=2,size=100) \n",
    "model.save('Word2vec_model20180108.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建词语字典，并返回word2vec模型中词语的索引，词向量\n",
    "def create_dictionaries(model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()} #词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()} #词语的词向量\n",
    "    return w2indx, w2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#词向量模型训练\n",
    "index_dict, word_vectors= create_dictionaries(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#每个APP描述对应的词索引\n",
    "discribe_index = []\n",
    "for sentence in app_discribe_list:\n",
    "    new_txt = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            new_txt.append(w2indx[word])\n",
    "        except:\n",
    "            new_txt.append(0)\n",
    "    discribe_index.append(new_txt)\n",
    "discribe_index = sequence.pad_sequences(discribe_index, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_array = np.array(app_tags_matrix)\n",
    "n_symbols = len(index_dict) + 1  #所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "embedding_weights = np.zeros((n_symbols, 100)) #索引为0的词语，词向量全为0\n",
    "for word, index in index_dict.items(): #从索引为1的词语开始，对每个词语对应其词向量\n",
    "    embedding_weights[index, :] = word_vectors[word]\n",
    "x_train, x_test, y_train, y_test = train_test_split(discribe_index, label_array, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义网络结构\n",
    "def train_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=100, input_dim=n_symbols, mask_zero=True,weights=[embedding_weights], input_length=100)) \n",
    "    model.add(LSTM(output_dim=100, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4703))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, batch_size=1000, verbose=1,epochs=3, shuffle=True, validation_split=0.2)\n",
    "    score = model.evaluate(x_test, y_test, batch_size=100)\n",
    "    yaml_string = model.to_yaml()\n",
    "    model.save_weights('lstm.h5')\n",
    "    print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", units=100, recurrent_activation=\"hard_sigmoid\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108394 samples, validate on 12044 samples\n",
      "Epoch 1/3\n",
      "108394/108394 [==============================] - 71s - loss: 0.6931 - acc: 1.0000 - val_loss: 0.6931 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "108394/108394 [==============================] - 60s - loss: 0.6931 - acc: 1.0000 - val_loss: 0.6930 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "108394/108394 [==============================] - 60s - loss: 0.6930 - acc: 1.0000 - val_loss: 0.6930 - val_acc: 1.0000\n",
      "30110/30110 [==============================] - 6s     \n",
      "Test score: [0.69297343492507935, 0.99997634471591146]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=100, input_dim=n_symbols, mask_zero=True,weights=[embedding_weights], input_length=100)) \n",
    "model.add(LSTM(output_dim=100, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4703))\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=1000, verbose=1,epochs=3, validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=1000)\n",
    "yaml_string = model.to_yaml()\n",
    "model.save_weights('lstm.h5')\n",
    "print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", units=100, recurrent_activation=\"hard_sigmoid\")`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96350 samples, validate on 24088 samples\n",
      "Epoch 1/3\n",
      "96350/96350 [==============================] - 57s - loss: 0.6931 - acc: 1.0000 - val_loss: 0.6931 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "96350/96350 [==============================] - 56s - loss: 0.6931 - acc: 1.0000 - val_loss: 0.6930 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "96350/96350 [==============================] - 56s - loss: 0.6930 - acc: 1.0000 - val_loss: 0.6930 - val_acc: 1.0000\n",
      "30110/30110 [==============================] - 27s    \n",
      "Test score: [0.69299240149520469, 0.99997630403588988]\n"
     ]
    }
   ],
   "source": [
    "train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
